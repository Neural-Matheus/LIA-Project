# ADR.002 — Chunking e limites de tokens para embeddings e contexto no RAG jurídico (Súmulas do STJ) — v2 (Sentence-Transformers)

## Context and Problem Statement

Vamos recuperar Súmulas do STJ por similaridade usando Sentence-Transformers (ST) multilíngues:

- `paraphrase-multilingual-MiniLM-L12-v2` (rápido/leve) e
- `paraphrase-multilingual-mpnet-base-v2` (mais pesado/preciso).

Precisamos particionar cada súmula em chunks que:

- respeitem o limite efetivo de subwords do tokenizer do modelo ST escolhido;
- preservem integridade do enunciado;
- mantenham IDs/offsets rastreáveis;
- se ajustem ao budget de contexto da LLM.

## Decision Drivers

- Recall@k alto para consultas curtas/técnicas (jurídico).
- Coesão semântica (não cortar enunciado).
- Eficiência (index compacto, poucas janelas).
- Simplicidade (heurística clara, baixa manutenção).
- Rastreabilidade (IDs estáveis + offsets).

## Considered Options

1. Documento inteiro sem chunking (truncamento).
2. Janela fixa (ex.: 384/64) ignorando estrutura.
3. Híbrido "structure-first" + fallback em janelas ✅
4. Segmentação por sentenças (pySBD) até o alvo.

## Decision Outcome

**Escolha:** (3) Híbrido "structure-first". Primeiro segmentamos por seções lógicas, e apenas se uma seção estourar o alvo aplicamos janela deslizante com overlap.

### Política de Chunking (atualizada p/ ST)

#### Seções (ordem)
1. **header:** SÚMULA {number} + topic (curto)
2. **enunciado** (sempre como chunk único se couber no limite)
3. **referencias_legislativas**
4. **orgao_julgador** + data_decisao + fonte
5. **excertos_precedentes** (geralmente mais longo → janelar)

#### Presets por modelo (alvo / overlap)
- **MiniLM:** target_size=384, overlap=64
- **MPNet:** target_size=320, overlap=48

**Obs.:** usamos o tokenizer do próprio modelo ST para contar tokens. Para o título do chunk ("SÚMULA N — …"), damos folga no max_seq_length do encoder (ex.: 352–416) ou descontamos o custo do título do target_size.

#### Limites
- **Capping por seção:** até 4 janelas por seção (max_windows_per_section=4).
- **Chunks mínimos:** descartar < 10 tokens.
- **Header único:** emitir no máx. 1× por súmula.

#### IDs e offsets
- **Seção curta:** "{number}#{section}" (ex.: 60#enunciado).
- **Janela:** "{number}#{section}@{start_token}-{end_token}".
- Guardar offsets de tokens e chars: token_start, token_end, char_start, char_end.

#### Texto do chunk
- Prepend de título curto (ex.: SÚMULA 60 — ENUNCIADO) + conteúdo.
- Normalização leve (trims e espaços), sem alterar o texto jurídico.

#### "Mini" enunciado (opcional)
- **enunciado_mini** ≤ 128 tokens para consultas muito curtas (boost no recall).

### Orçamento da LLM (retrieval → prompt)
- **Context window:** dependente da LLM (8k/16k/32k).
- **Budget recuperado (default):** ~4k tokens.
- **Top-k adaptativo:** ordenar por score; priorizar:
  - enunciado da(s) súmula(s) top,
  - excertos_precedentes (1–2 melhores),
  - referencias_legislativas e orgao/data/fonte (se curtos),
  - header (1×).

## Consequências

- Melhor recall sem perder coesão (enunciado intacto).
- Explainability com IDs/offsets.
- Index enxuto; fallback em janelas só quando precisa.

## Risks & Mitigações

- **Excertos muito longos** ⇒ muitos sub-chunks → cap 4 janelas/seção; extra só sob demanda.
- **Variação de tokenização entre modelos** ⇒ sempre chunkar com o tokenizer do encoder escolhido.
- **Token budget do título** ⇒ folga no max_seq_length ou descontar título do target_size.

## Implementation Sketch

- **Tokenizer** = do ST escolhido (MiniLM/MPNet).
- **chunk_section(text):** tokeniza; se tokens ≤ target_size ⇒ 1 janela; senão janelas (size, stride=size-overlap).
- IDs + offsets; JSONL final de chunks (corpus canônico).
- **Próxima fase:** embeddings + FAISS per-number (ADR.003 v2).

---
