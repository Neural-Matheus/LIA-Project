# ADR.007 — Camada de geração: do modelo local (HF) para Groq + prompt autobudget

## Context and Problem Statement

O primeiro POC usou modelo local HF (TinyLlama) para gerar respostas. Latência/qualidade ficaram limitadas. Precisamos de modelos melhores com janela de contexto grande, mas respeitando limites de TPM e service tier.

## Decision Drivers

- Qualidade da saída (clareza jurídica + citações curtas)
- Janela grande para empacotar mais contexto quando útil
- Simplicidade de integração no Colab
- Resiliência a limites de provedor (TPM/tier)

## Considered Options

- Somente local HF (Tiny/Small)
- **Groq Chat Completions (Llama 3.x)** ✅
- Misto (fallback local quando rate limit)

## Decision Outcome

- **Gerador padrão:** Groq llama-3.1-8b-instant (rápido, barato) e opção llama-3.3-70b-versatile (qualidade)
- **Tokenização auxiliar para budget:** tokenizer HF leve (TinyLlama ou Llama-3) só para contagem/truncagem
- **Prompt autobudget:** função que aloca contexto até um teto (ajustado ao TPM e max_completion_tokens), truncando de forma inteligente (enunciado completo > evidências)

## Consequences

- Melhor qualidade de resposta, mantendo citabilidade (IDs dos chunks)
- Tolerância a 413 (TPM exceeded) com retry que reduz budget

## Risks & Mitigations

- **Risco:** 400 "service_tier" inválido  
  **Mitigação:** não enviar service_tier quando não suportado
- **Risco:** Diferença de tokenização entre régua HF e modelo remoto  
  **Mitigação:** usar margem de segurança
