# ADR.008 — Avaliação offline, métricas e observabilidade

## Context and Problem Statement

Precisamos acompanhar progresso e evitar regressões. Além de testes manuais, definimos um protocolo offline de avaliação.

## Decision Drivers

- Métrica direta para "achar a súmula correta"
- Cobrir dois cenários: (A) query é o enunciado/mini; (B) query é excerto/ementa
- Facilidade de repetir no Colab

## Considered Options

- Só Recall@k por chunk
- **Recall@k por número (per-number)** ✅
- MRR/NDCG opcionais depois

## Decision Outcome

- **Métrica principal:** Recall@k per-number (k=1/3/5/10)
- **Cenários:**
  - **A (auto-match):** enunciado_mini → índice (demais seções)
  - **B (realista):** enunciados contra outras seções; e excertos contra o índice completo
- **Registros:** tempo de encode e de busca; distribuição de scores top-1 vs top-2; casos difíceis

## Observed Baselines (exemplos da nossa fase)

- **A (auto-match por número):** R@1 ≈ 0.24–0.33; R@10 ≈ 0.55
- **B (mínimo robusto com per-number + pesos + reranker):** melhora estável nos "casos confusos" (p.ex., Súmula 72 vs 229)

**Obs.:** valores variam conforme chunking, pesos e encoder. Usar notebook de avaliação para congelar checkpoints.

## Risks & Mitigations

- **Risco:** Overfit em dev set pequeno  
  **Mitigação:** dividir por temáticas/tempos; validação estratificada
- **Risco:** Mudanças de chunking/encoder quebrando comparabilidade  
  **Mitigação:** versionar manifest.json + datasets