{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "WSkl4S_mOfUc"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import os, sys, subprocess, json, re, time\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple, Optional, Any\n",
        "from collections import defaultdict\n",
        "from sentence_transformers import CrossEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "UnmngYSlPdny"
      },
      "outputs": [],
      "source": [
        "OUT_DIR = \"/content/faiss_per_number\"\n",
        "PROMPT_TOKENIZER_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "def ensure(pkgs: List[str]):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs)\n",
        "\n",
        "try:\n",
        "    import torch, faiss\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "except Exception:\n",
        "    ensure([\"torch\", \"faiss-cpu\", \"sentence-transformers\"])\n",
        "    import torch, faiss\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "\n",
        "try:\n",
        "    from transformers import AutoTokenizer\n",
        "except Exception:\n",
        "    ensure([\"transformers\", \"accelerate\", \"sentencepiece\"])\n",
        "    from transformers import AutoTokenizer\n",
        "\n",
        "try:\n",
        "    import groq\n",
        "except Exception:\n",
        "    ensure([\"groq\"])\n",
        "    import groq\n",
        "from groq import Groq\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "5nRCHbmKPwKS"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class EncoderConfig:\n",
        "    model_name: str\n",
        "    max_seq_length: int = 384\n",
        "    batch_size: int = 64\n",
        "    device: Optional[str] = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qomHLFHIPd0D"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GroqConfig:\n",
        "    model: str = \"llama-3.1-8b-instant\"\n",
        "    temperature: float = 0.0\n",
        "    top_p: float = 1.0\n",
        "    max_completion_tokens: int = 700\n",
        "    service_tier: Optional[str] = \"auto\"\n",
        "    timeout: Optional[float] = 60.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "d-5tb4LcPyQk"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class BuildPaths:\n",
        "    out_dir: str\n",
        "    chunk_faiss: str\n",
        "    chunk_docstore: str\n",
        "    manifest: str\n",
        "    numbers_faiss: str\n",
        "    numbers_meta: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Py5P_GXTPzpU"
      },
      "outputs": [],
      "source": [
        "def make_paths(out_dir: str) -> BuildPaths:\n",
        "    p = Path(out_dir); p.mkdir(parents=True, exist_ok=True)\n",
        "    return BuildPaths(\n",
        "        out_dir=str(p),\n",
        "        chunk_faiss=str(p / \"chunks.faiss\"),\n",
        "        chunk_docstore=str(p / \"docstore.json\"),\n",
        "        manifest=str(p / \"manifest.json\"),\n",
        "        numbers_faiss=str(p / \"numbers.faiss\"),\n",
        "        numbers_meta=str(p / \"numbers_meta.json\"),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Eip5cGmWP1pm"
      },
      "outputs": [],
      "source": [
        "class STEncoder:\n",
        "    def __init__(self, cfg: EncoderConfig):\n",
        "        dev = cfg.device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = SentenceTransformer(cfg.model_name, device=dev)\n",
        "        self.model.max_seq_length = cfg.max_seq_length\n",
        "        self.batch_size = cfg.batch_size\n",
        "    def encode(self, texts: List[str]) -> np.ndarray:\n",
        "        try:\n",
        "            X = self.model.encode(texts, batch_size=self.batch_size, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False).astype(np.float32)\n",
        "        except TypeError:\n",
        "            X = self.model.encode(texts, batch_size=self.batch_size, convert_to_numpy=True, show_progress_bar=False).astype(np.float32)\n",
        "            faiss.normalize_L2(X)\n",
        "        return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xY1xH3f1P3DN"
      },
      "outputs": [],
      "source": [
        "class ChunkFaissIndex:\n",
        "    def __init__(self):\n",
        "        self.index: faiss.Index = None\n",
        "        self.ids: List[str] = []\n",
        "        self.texts: List[str] = []\n",
        "        self.numbers: List[str] = []\n",
        "        self.sections: List[str] = []\n",
        "    @classmethod\n",
        "    def load(cls, paths: BuildPaths) -> \"ChunkFaissIndex\":\n",
        "        obj = cls()\n",
        "        obj.index = faiss.read_index(paths.chunk_faiss)\n",
        "        doc = json.loads(Path(paths.chunk_docstore).read_text(encoding=\"utf-8\"))\n",
        "        obj.ids = doc[\"ids\"]; obj.texts = doc[\"texts\"]; obj.numbers = doc[\"numbers\"]; obj.sections = doc[\"sections\"]\n",
        "        return obj\n",
        "    def search(self, Q: np.ndarray, top_k: int) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        faiss.normalize_L2(Q)\n",
        "        return self.index.search(Q.astype(np.float32), top_k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "C6VN31dSP4XW"
      },
      "outputs": [],
      "source": [
        "class NumberFaissIndex:\n",
        "    def __init__(self):\n",
        "        self.index: faiss.Index = None\n",
        "        self.numbers: List[str] = []\n",
        "        self.groups: Dict[str, List[int]] = {}\n",
        "    @classmethod\n",
        "    def load(cls, paths: BuildPaths) -> Optional[\"NumberFaissIndex\"]:\n",
        "        if not Path(paths.numbers_faiss).exists() or not Path(paths.numbers_meta).exists():\n",
        "            return None\n",
        "        obj = cls()\n",
        "        obj.index = faiss.read_index(paths.numbers_faiss)\n",
        "        meta = json.loads(Path(paths.numbers_meta).read_text(encoding=\"utf-8\"))\n",
        "        obj.numbers = meta[\"numbers\"]; obj.groups = {k: list(v) for k, v in meta[\"groups\"].items()}\n",
        "        return obj\n",
        "    def search(self, Q: np.ndarray, top_k: int) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        faiss.normalize_L2(Q)\n",
        "        return self.index.search(Q.astype(np.float32), top_k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "pE50Lcd2P6fv"
      },
      "outputs": [],
      "source": [
        "SECTION_WEIGHT = {\n",
        "    \"enunciado\": 1.40,\n",
        "    \"enunciado_mini\": 1.30,\n",
        "    \"referencias_legislativas\": 1.05,\n",
        "    \"excertos_precedentes\": 1.00,\n",
        "    \"orgao_data_fonte\": 0.90,\n",
        "    \"header\": 0.85,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SCtuj6X-P8-s"
      },
      "outputs": [],
      "source": [
        "def aggregate_by_number(js: List[int], sims: np.ndarray, numbers: List[str], sections: List[str], pool: str = \"sum_sqrt\"):\n",
        "    if pool == \"max\":\n",
        "        tmp = defaultdict(float)\n",
        "        for j, s in zip(js, sims):\n",
        "            w = SECTION_WEIGHT.get(sections[j], 1.0)\n",
        "            tmp[numbers[j]] = max(tmp[numbers[j]], float(s) * w)\n",
        "        return sorted(tmp.items(), key=lambda x: x[1], reverse=True)\n",
        "    agg = defaultdict(float); counts = defaultdict(int)\n",
        "    for j, s in zip(js, sims):\n",
        "        w = SECTION_WEIGHT.get(sections[j], 1.0)\n",
        "        agg[numbers[j]] += float(s) * w\n",
        "        counts[numbers[j]] += 1\n",
        "    for n in list(agg.keys()):\n",
        "        agg[n] = agg[n] / max(1.0, np.sqrt(counts[n]))\n",
        "    return sorted(agg.items(), key=lambda x: x[1], reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "56EZ3K9-P-fy"
      },
      "outputs": [],
      "source": [
        "class ChunkIdxView:\n",
        "    def __init__(self, index: ChunkFaissIndex):\n",
        "        self.index = index.index\n",
        "        self.ids = index.ids\n",
        "        self.texts = index.texts\n",
        "        self.numbers = index.numbers\n",
        "        self.sections = index.sections\n",
        "    def search(self, Q: np.ndarray, top_k: int):\n",
        "        return self.index.search(Q.astype(np.float32), top_k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "QdxpyYcTYX2s"
      },
      "outputs": [],
      "source": [
        "def _restrict_idxs_to_numbers(idxs: np.ndarray, numbers_list: List[str], allow_set: set) -> np.ndarray:\n",
        "    out = []\n",
        "    for row in idxs:\n",
        "        keep = [j for j in row if j != -1 and numbers_list[j] in allow_set]\n",
        "        if not keep:\n",
        "            keep = [row[0]] if row.size > 0 else [-1]\n",
        "        out.append(np.array(keep, dtype=np.int64))\n",
        "    return np.stack(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "vdKTCueYQA9s"
      },
      "outputs": [],
      "source": [
        "class PerNumberRetriever:\n",
        "    def __init__(self, out_dir: str):\n",
        "        self.paths = make_paths(out_dir)\n",
        "        self.manifest = json.loads(Path(self.paths.manifest).read_text(encoding=\"utf-8\"))\n",
        "        self.chunk_idx = ChunkIdxView(ChunkFaissIndex.load(self.paths))\n",
        "        self.number_idx = NumberFaissIndex.load(self.paths)\n",
        "    def encode_queries(self, texts: List[str], enc_cfg: Optional[EncoderConfig] = None) -> np.ndarray:\n",
        "        cfg = enc_cfg or EncoderConfig(model_name=self.manifest[\"model_name\"])\n",
        "        cfg = EncoderConfig(model_name=self.manifest.get(\"model_name\", cfg.model_name), max_seq_length=int(self.manifest.get(\"max_seq_length\", 384)), batch_size=cfg.batch_size, device=cfg.device)\n",
        "        enc = STEncoder(cfg)\n",
        "        return enc.encode(texts).astype(np.float32)\n",
        "    def search_two_stage(self, Q: np.ndarray, top_numbers: int = 20, top_chunks_per_query: int = 200, pool: str = \"sum_sqrt\") -> List[List[Tuple[str, float]]]:\n",
        "        sims, idxs = self.chunk_idx.search(Q, top_chunks_per_query)\n",
        "        out = []\n",
        "        for i in range(Q.shape[0]):\n",
        "            js = idxs[i].tolist()\n",
        "            sc = sims[i].tolist()\n",
        "            pairs = aggregate_by_number(js, sc, self.chunk_idx.numbers, self.chunk_idx.sections, pool=pool)\n",
        "            out.append(pairs[:top_numbers])\n",
        "        return out\n",
        "    def search_two_stage_hybrid(\n",
        "        self,\n",
        "        Q: np.ndarray,\n",
        "        coarse_top_numbers: int = 50,\n",
        "        refine_top_chunks: int = 300,\n",
        "        final_top_numbers: int = 20,\n",
        "        pool: str = \"sum_sqrt\"\n",
        "    ) -> List[List[Tuple[str, float]]]:\n",
        "        if self.number_idx is None:\n",
        "            return self.search_two_stage(Q, top_numbers=final_top_numbers, top_chunks_per_query=refine_top_chunks, pool=pool)\n",
        "\n",
        "        sims_num, idxs_num = self.number_idx.search(Q, coarse_top_numbers)\n",
        "        out = []\n",
        "        for i in range(Q.shape[0]):\n",
        "            allow = { self.number_idx.numbers[j] for j in idxs_num[i] if j != -1 }\n",
        "            sims_ck, idxs_ck = self.chunk_idx.search(Q[i:i+1], refine_top_chunks)\n",
        "            idxs_filt = _restrict_idxs_to_numbers(idxs_ck, self.chunk_idx.numbers, allow)\n",
        "            sc = sims_ck[0].tolist()\n",
        "            js = idxs_filt[0].tolist()\n",
        "            pairs = aggregate_by_number(js, sc, self.chunk_idx.numbers, self.chunk_idx.sections, pool=pool)\n",
        "            out.append(pairs[:final_top_numbers])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NMW9kE9GQCh3"
      },
      "outputs": [],
      "source": [
        "def pick_evidence_from_idxs(retriever: PerNumberRetriever, number: str, top_chunks_global_idxs, max_chunks: int = 3):\n",
        "    prefer = [\"enunciado\", \"enunciado_mini\", \"excertos_precedentes\", \"referencias_legislativas\", \"orgao_data_fonte\", \"header\"]\n",
        "    selected, seen = [], set()\n",
        "    for sec in prefer:\n",
        "        for j in top_chunks_global_idxs:\n",
        "            if j == -1 or j in seen:\n",
        "                continue\n",
        "            if retriever.chunk_idx.numbers[j] == number and retriever.chunk_idx.sections[j] == sec:\n",
        "                selected.append({\"id\": retriever.chunk_idx.ids[j], \"section\": retriever.chunk_idx.sections[j], \"text\": retriever.chunk_idx.texts[j]})\n",
        "                seen.add(j)\n",
        "                if len(selected) >= max_chunks:\n",
        "                    return selected\n",
        "    return selected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "tWPZO0tGYsFm"
      },
      "outputs": [],
      "source": [
        "class Reranker:\n",
        "    def __init__(self, model_id: str = \"mixedbread-ai/mxbai-rerank-xsmall-v1\", device: Optional[str] = None, max_length: int = 512):\n",
        "        self.model = CrossEncoder(model_id, device=device or (\"cuda\" if torch.cuda.is_available() else \"cpu\"), max_length=max_length)\n",
        "    def rerank_numbers(self, query: str, hits: List[Dict[str, Any]], k: int = 5) -> List[Dict[str, Any]]:\n",
        "        if not hits: return hits\n",
        "        pairs = []\n",
        "        for h in hits:\n",
        "            text = (h.get(\"enunciado\") or \"\").strip()\n",
        "            if not text and h.get(\"evidences\"):\n",
        "                text = h[\"evidences\"][0].get(\"text\",\"\")\n",
        "            pairs.append([query, text])\n",
        "        scores = self.model.predict(pairs, convert_to_numpy=True)\n",
        "        order = np.argsort(-scores)[:k]\n",
        "        return [hits[i] | {\"rerank_score\": float(scores[i])} for i in order]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "lF7B_p2CQD9W"
      },
      "outputs": [],
      "source": [
        "def gather_hits_for_query(query: str, retriever: PerNumberRetriever, enc_cfg: Optional[EncoderConfig] = None, top_numbers: int = 5, top_chunks_per_query: int = 200, max_evidence_per_number: int = 3) -> List[Dict[str, Any]]:\n",
        "    Q = retriever.encode_queries([query], enc_cfg=enc_cfg)\n",
        "    ranked = retriever.search_two_stage_hybrid(\n",
        "        Q,\n",
        "        coarse_top_numbers=60,\n",
        "        refine_top_chunks=300,\n",
        "        final_top_numbers=top_numbers,\n",
        "        pool=\"sum_sqrt\"\n",
        "    )[0]\n",
        "    sims, idxs = retriever.chunk_idx.search(Q, top_k=top_chunks_per_query)\n",
        "    results = []\n",
        "    for number, score in ranked[:top_numbers]:\n",
        "        enun = next((retriever.chunk_idx.texts[i] for i, num in enumerate(retriever.chunk_idx.numbers) if num == number and retriever.chunk_idx.sections[i] == \"enunciado\"), \"\")\n",
        "        ev = pick_evidence_from_idxs(retriever, number=number, top_chunks_global_idxs=idxs[0], max_chunks=max_evidence_per_number)\n",
        "        results.append({\"number\": number, \"score\": float(score), \"enunciado\": enun, \"evidences\": ev})\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "TYWZqVAUQF-y"
      },
      "outputs": [],
      "source": [
        "def count_tokens(text: str, tokenizer) -> int:\n",
        "    return len(tokenizer.encode(text or \"\", add_special_tokens=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZGbXjIKzQIEu"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compact_hits_autobudget(hits: List[Dict[str, Any]], tokenizer, token_budget: int = 12000, system_overhead: int = 300, query_overhead: int = 600, max_numbers_cap: int = 12, max_evidence_per_number_cap: int = 6) -> List[Dict[str, Any]]:\n",
        "    used = system_overhead + query_overhead\n",
        "    out: List[Dict[str, Any]] = []\n",
        "    for h in hits[:max_numbers_cap]:\n",
        "        enun = (h.get(\"enunciado\") or \"\").strip()\n",
        "        enun_cost = count_tokens(f\"SÚMULA {h['number']} — ENUNCIADO:\\n{enun}\", tokenizer)\n",
        "        evs = h.get(\"evidences\", [])[:max_evidence_per_number_cap]\n",
        "        ev_costs = []\n",
        "        for ev in evs:\n",
        "            sec = ev.get(\"section\", \"\")\n",
        "            txt = (ev.get(\"text\") or \"\").strip()\n",
        "            ev_costs.append((ev, count_tokens(f\"[{ev['id']} | {sec}]\\n{txt}\", tokenizer)))\n",
        "        need = enun_cost + sum(c for _, c in ev_costs)\n",
        "        if used + need <= token_budget:\n",
        "            out.append({\"number\": h[\"number\"], \"score\": float(h.get(\"score\", 0.0)), \"enunciado\": enun, \"evidences\": evs})\n",
        "            used += need\n",
        "            continue\n",
        "        if used + enun_cost > token_budget:\n",
        "            break\n",
        "        acc_evs, acc_cost = [], enun_cost\n",
        "        for ev, c in ev_costs:\n",
        "            if used + acc_cost + c <= token_budget:\n",
        "                acc_evs.append(ev); acc_cost += c\n",
        "            else:\n",
        "                room = token_budget - (used + acc_cost)\n",
        "                if room > 30:\n",
        "                    ids = tokenizer.encode(ev.get(\"text\") or \"\", add_special_tokens=False)[:room]\n",
        "                    txt_trunc = tokenizer.decode(ids)\n",
        "                    if txt_trunc.strip():\n",
        "                        ev_trunc = {**ev, \"text\": txt_trunc}\n",
        "                        acc_evs.append(ev_trunc)\n",
        "                        acc_cost += room\n",
        "                break\n",
        "        out.append({\"number\": h[\"number\"], \"score\": float(h.get(\"score\", 0.0)), \"enunciado\": enun, \"evidences\": acc_evs})\n",
        "        used += acc_cost\n",
        "        if used >= token_budget:\n",
        "            break\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8CLtf84RQJk0"
      },
      "outputs": [],
      "source": [
        "_SECTION_PT = {\n",
        "    \"enunciado\": \"Enunciado\",\n",
        "    \"enunciado_mini\": \"Enunciado (mini)\",\n",
        "    \"excertos_precedentes\": \"Excerto dos Precedentes\",\n",
        "    \"referencias_legislativas\": \"Referências Legislativas\",\n",
        "    \"orgao_data_fonte\": \"Órgão Julgador / Data / Fonte\",\n",
        "    \"header\": \"Cabeçalho\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "aUG5iW-9QK1z"
      },
      "outputs": [],
      "source": [
        "def _sec_label(sec: str) -> str:\n",
        "    return _SECTION_PT.get(sec, sec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "LjSl02Z4QMT2"
      },
      "outputs": [],
      "source": [
        "_CASE_RE = re.compile(r'\\b(REsp|AgRg|HC|EDcl|AgInt|RMS|AREsp)\\b[^)\\]\\n]{0,120}')\n",
        "def extract_case_citation(text: str) -> str:\n",
        "    m = _CASE_RE.search(text or \"\");\n",
        "    if not m: return \"\"\n",
        "    span_start = m.start(); cut = text[span_start: span_start + 160]\n",
        "    cut = re.split(r'[\\)\\n]', cut, maxsplit=1)[0]\n",
        "    return cut.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "wSEFcrxUQNie"
      },
      "outputs": [],
      "source": [
        "def build_prompt_from_compact_narrative(query: str, compact_hits_list: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
        "    system = (\n",
        "        \"Você é um assistente jurídico especializado em Súmulas do STJ. \"\n",
        "        \"Identifique a(s) Súmula(s) aplicável(is) ao trecho, cite o número e explique brevemente. \"\n",
        "        \"Ao citar evidências, indique a seção e o [chunk_id]; quando houver, mencione a referência do precedente.\"\n",
        "    )\n",
        "    ctx_lines = []\n",
        "    for h in compact_hits_list:\n",
        "        num = h[\"number\"]; enun = (h.get(\"enunciado\") or \"\").strip()\n",
        "        ctx_lines.append(f\"SÚMULA {num} — ENUNCIADO:\\n{(enun if enun else '(sem enunciado)')}\")\n",
        "        for ev in h.get(\"evidences\", []):\n",
        "            sec = _sec_label(ev.get(\"section\", \"\")); text = (ev.get(\"text\") or \"\").strip()\n",
        "            case = extract_case_citation(text)\n",
        "            bits = [f\"seção: {sec}\", f\"[{ev['id']}\"] + ([f\"originado de {case}\"] if case else [])\n",
        "            ctx_lines.append(f\"Evidência (Súmula {num}) — {'; '.join(bits)}:\\n{text}\")\n",
        "    context_block = \"\\n\\n\".join(ctx_lines)\n",
        "    user = (\n",
        "        f\"Trecho do usuário:\\n---\\n{query.strip()}\\n---\\n\\n\"\n",
        "        f\"Contexto:\\n{context_block}\\n\\n\"\n",
        "        \"Responda:\\n\"\n",
        "        \"1) Diga qual(is) Súmula(s) se alinham ao trecho, no formato textual: \"\n",
        "        \"\\\"Este é um trecho da Súmula NNN (seção: ...) que veio do [chunk_id]\\\".\\n\"\n",
        "        \"2) Explique em 2–4 frases, citando trechos entre aspas com [chunk_id].\\n\"\n",
        "        \"3) Se houver incerteza, reporte top-3 candidatas.\\n\"\n",
        "        \"4) Final: 'Súmula aplicável: NNN'.\"\n",
        "    )\n",
        "    return [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrMVw2TUS5k3",
        "outputId": "6ea3a7c1-9884-47ce-8ba3-cdb4151f8e22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ok\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(\"ok\" if os.environ.get(\"GROQ_API_KEY\") else \"missing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "AtNen7FRQPKa"
      },
      "outputs": [],
      "source": [
        "class GroqChat:\n",
        "    def __init__(self, cfg: GroqConfig):\n",
        "        api_key = os.environ.get(\"GROQ_API_KEY\")\n",
        "        if not api_key:\n",
        "            raise RuntimeError(\"Set GROQ_API_KEY in environment.\")\n",
        "        self.client = Groq(api_key=api_key, timeout=cfg.timeout)\n",
        "        self.cfg = cfg\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(PROMPT_TOKENIZER_ID, use_fast=True)\n",
        "\n",
        "    def chat(self, messages):\n",
        "        kwargs = dict(\n",
        "            model=self.cfg.model,\n",
        "            messages=messages,\n",
        "            temperature=self.cfg.temperature,\n",
        "            top_p=self.cfg.top_p,\n",
        "            max_completion_tokens=self.cfg.max_completion_tokens,\n",
        "            stream=False,\n",
        "        )\n",
        "        if self.cfg.service_tier:\n",
        "            kwargs[\"service_tier\"] = self.cfg.service_tier\n",
        "        resp = self.client.chat.completions.create(**kwargs)\n",
        "        return resp.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "xLMTmgQmQQ0V"
      },
      "outputs": [],
      "source": [
        "_LLM_CACHE: Dict[str, Any] = {}\n",
        "def get_llm(gen_cfg: GroqConfig) -> GroqChat:\n",
        "    key = f\"{gen_cfg.model}\"\n",
        "    if key not in _LLM_CACHE:\n",
        "        _LLM_CACHE[key] = GroqChat(gen_cfg)\n",
        "    return _LLM_CACHE[key]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "4N3ACJXaTstR"
      },
      "outputs": [],
      "source": [
        "def count_messages_tokens(messages, tokenizer) -> int:\n",
        "    text = \"\"\n",
        "    for m in messages:\n",
        "        role = m.get(\"role\",\"\")\n",
        "        content = m.get(\"content\",\"\")\n",
        "        text += f\"<|{role}|>\\n{content}\\n\"\n",
        "    return len(tokenizer.encode(text, add_special_tokens=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "aYMgss5yXC7S"
      },
      "outputs": [],
      "source": [
        "def load_retriever(out_dir: str = OUT_DIR) -> Tuple[PerNumberRetriever, EncoderConfig]:\n",
        "    paths = make_paths(out_dir)\n",
        "    mani = json.loads(Path(paths.manifest).read_text(encoding=\"utf-8\"))\n",
        "    enc_cfg = EncoderConfig(model_name=mani[\"model_name\"], max_seq_length=int(mani.get(\"max_seq_length\", 384)))\n",
        "    retriever = PerNumberRetriever(out_dir)\n",
        "    return retriever, enc_cfg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "kz_GDCSnXEZ6"
      },
      "outputs": [],
      "source": [
        "retriever, enc_cfg = load_retriever(OUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "Yqt4llH0QXg9"
      },
      "outputs": [],
      "source": [
        "def rag_answer(\n",
        "    query: str,\n",
        "    retriever: PerNumberRetriever,\n",
        "    enc_cfg: Optional[EncoderConfig] = None,\n",
        "    gen_cfg: GroqConfig = GroqConfig(),\n",
        "    top_numbers: int = 8,\n",
        "    top_chunks_per_query: int = 160,\n",
        "    max_evidence_per_number: int = 3,\n",
        "    token_budget: Optional[int] = None,\n",
        "    tpm_limit: int = 6000,\n",
        "    safety_tokens: int = 200,\n",
        "    use_reranker: bool = True,\n",
        "    rerank_keep: int = 5\n",
        ") -> Dict[str, Any]:\n",
        "    hits_all = gather_hits_for_query(query, retriever, enc_cfg, top_numbers, top_chunks_per_query, max_evidence_per_number)\n",
        "    if use_reranker:\n",
        "        rr = Reranker()\n",
        "        hits_all = rr.rerank_numbers(query, hits_all, k=min(rerank_keep, len(hits_all)))\n",
        "\n",
        "    llm = get_llm(gen_cfg)\n",
        "    prompt_limit = token_budget if token_budget is not None else max(512, tpm_limit - int(gen_cfg.max_completion_tokens) - safety_tokens)\n",
        "    hits_compact = compact_hits_autobudget(hits_all, tokenizer=llm.tokenizer, token_budget=prompt_limit)\n",
        "    messages = build_prompt_from_compact_narrative(query, hits_compact)\n",
        "    if count_messages_tokens(messages, llm.tokenizer) > prompt_limit:\n",
        "        hits_compact = compact_hits_autobudget(hits_all, tokenizer=llm.tokenizer, token_budget=max(512, int(prompt_limit*0.9)))\n",
        "        messages = build_prompt_from_compact_narrative(query, hits_compact)\n",
        "    answer = llm.chat(messages)\n",
        "    return {\"answer\": answer, \"hits\": hits_compact, \"messages\": messages}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "V6pfVnD0QYnh"
      },
      "outputs": [],
      "source": [
        "def load_retriever(out_dir: str = OUT_DIR) -> Tuple[PerNumberRetriever, EncoderConfig]:\n",
        "    paths = make_paths(out_dir)\n",
        "    mani = json.loads(Path(paths.manifest).read_text(encoding=\"utf-8\"))\n",
        "    enc_cfg = EncoderConfig(model_name=mani[\"model_name\"], max_seq_length=int(mani.get(\"max_seq_length\", 384)))\n",
        "    retriever = PerNumberRetriever(out_dir)\n",
        "    return retriever, enc_cfg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "nCCBx02bQZ8i"
      },
      "outputs": [],
      "source": [
        "QUERY = \"\"\"\"[...] REFIS. LEGITIMIDADE DA EXCLUSÃO POR MEIO DO DIÁRIO OFICIAL E DA INTERNET.\n",
        "AFASTAMENTO DA LEGISLAÇÃO SUBSIDIÁRIA (LEI 9.784/99). [...] Nos termos do art. 69 da Lei 9.784/99,\n",
        "'os processos administrativos específicos continuarão a reger-se por lei própria, aplicando-se-lhes\n",
        "apenas subsidiariamente os preceitos desta Lei'. Considerando que o REFIS é regido especificamente\n",
        "pela Lei 9.964/2000, a sua incidência afasta a aplicação da norma subsidiária (Lei 9.784/99). 2. Não há\n",
        "ilegalidade na exclusão do REFIS sem a intimação pessoal do contribuinte, efetuando-se a notificação\n",
        "por meio do Diário Oficial e da Internet, nos termos do art. 9º, III, da Lei 9.964/2000, c/c o art. 5º da\n",
        "Resolução 20/2001 do Comitê Gestor do Programa. [...]\" (AgRg no Ag 902614 PR, Rel. Ministra DENISE\n",
        "ARRUDA, PRIMEIRA TURMA, julgado em 13/11/2007, DJ 12/12/2007, p. 397)\"\"\"\n",
        "QUERY = \" \".join(QUERY.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsQrUFwdS8ZE",
        "outputId": "944137b4-8d2d-4362-d764-de8571799181"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Este é um trecho da Súmula 355 (seção: Excerto dos Precedentes; [355#excertos_precedentes@272-592; originado de AgRg no Ag 902614 PR, Rel. Ministra DENISE ARRUDA, PRIMEIRA TURMA, julgado em 13/11/2007, DJ 12/12/2007, p. 397]) que veio do [272-592].\n",
            "Este é um trecho da Súmula 355 (seção: Excerto dos Precedentes; [355#excertos_precedentes@0-320; originado de REsp 842906 DF, Rel. Ministra ELIANA CALMON, SEGUNDA TURMA, julgado em 06/05/2008, DJe: 19/05/2008]) que veio do [0-320].\n",
            "Este é um trecho da Súmula 355 (seção: Excerto dos Precedentes; [355#excertos_precedentes@816-1136; originado de REsp 761128 RS, Rel. Ministro CASTRO MEIRA, SEGUNDA TURMA, julgado em 17/05/2007, DJ 29/05/2007, p. 274]) que veio do [816-1136].\n",
            "\n",
            "2. A Súmula 355 se alinha ao trecho, pois aborda a validade da notificação do ato de exclusão do programa de recuperação fiscal do Refis pelo Diário Oficial ou pela Internet. O trecho cita a jurisprudência pacífica da Primeira e da Segunda Turma do STJ, que entende que a Lei 9.784/99 não se aplica ao procedimento de exclusão do REFIS,\n",
            "\n",
            "Candidates:\n",
            "Súmula 355 | score=1.872 | rerank=0.000\n",
            "  - 355#excertos_precedentes@272-592 | excertos_precedentes\n",
            "  - 355#excertos_precedentes@0-320 | excertos_precedentes\n",
            "Súmula 633 | score=1.539 | rerank=0.000\n",
            "  - 633#excertos_precedentes@816-1136 | excertos_precedentes\n",
            "  - 633#excertos_precedentes@544-864 | excertos_precedentes\n",
            "Súmula 538 | score=1.530 | rerank=0.000\n",
            "  - 538#excertos_precedentes@272-592 | excertos_precedentes\n"
          ]
        }
      ],
      "source": [
        "retriever, enc_cfg = load_retriever(OUT_DIR)\n",
        "gen_cfg = GroqConfig(model=\"llama-3.1-8b-instant\", temperature=0.0, max_completion_tokens=400, service_tier=\"on_demand\")\n",
        "\n",
        "res = rag_answer(\n",
        "    QUERY, retriever, enc_cfg, gen_cfg,\n",
        "    top_numbers=10,\n",
        "    top_chunks_per_query=200,\n",
        "    max_evidence_per_number=3,\n",
        "    tpm_limit=6000,\n",
        "    use_reranker=True,\n",
        "    rerank_keep=5\n",
        ")\n",
        "\n",
        "print(res[\"answer\"])\n",
        "print(\"\\nCandidates:\")\n",
        "for h in res[\"hits\"]:\n",
        "    print(f\"Súmula {h['number']} | score={h.get('score',0):.3f} | rerank={h.get('rerank_score',0):.3f}\")\n",
        "    for ev in h[\"evidences\"][:2]:\n",
        "        print(\"  -\", ev[\"id\"], \"|\", ev[\"section\"])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
